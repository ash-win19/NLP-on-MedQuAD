{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ebfc330",
   "metadata": {
    "id": "3ebfc330"
   },
   "source": [
    "# MedQuAD â€” Milestone 2 Progress Notebook (End-to-End)\n",
    "This notebook parses MedQuAD XML, performs EDA, creates leak-safe splits, and provides runnable baselines for:\n",
    "1) **Abstractive Answer Generation** (T5)  \n",
    "2) **Paraphrase Detection** (Siamese encoder)  \n",
    "3) **Retrieval-Augmented QA (RAG)** with BM25 + FAISS + Generator\n",
    "\n",
    "**Usage**\n",
    "1. Clone the dataset: `git clone https://github.com/abachaa/MedQuAD`\n",
    "2. Set `MEDQUAD_ROOT` below to that path.\n",
    "3. Run cells in order. Heavy training cells are commented for quick tests; uncomment on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ec55e09",
   "metadata": {
    "id": "7ec55e09"
   },
   "outputs": [],
   "source": [
    "# Optional: install dependencies in a fresh environment\n",
    "# !pip install -q lxml pandas scikit-learn sentence-transformers transformers datasets evaluate rank-bm25 faiss-cpu nltk matplotlib seaborn rouge-score\n",
    "\n",
    "import os, re, glob, warnings\n",
    "from urllib.parse import urlparse\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ðŸ‘‰ EDIT THIS PATH to your local MedQuAD clone (e.g., '/content/MedQuAD' in Colab)\n",
    "MEDQUAD_ROOT = \"/Users/ashwinshanmugam/Documents/UIUC/University Courses/Fall 2025/IS 567 Text Mining/Project/NLP-on-MedQuAD/data\"\n",
    "OUT_DIR = \"./artifacts-medquad\"; os.makedirs(OUT_DIR, exist_ok=True)\n",
    "MEDQUAD_PARQUET = os.path.join(OUT_DIR, \"medquad.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c90f674",
   "metadata": {
    "id": "1c90f674"
   },
   "source": [
    "## 1) Ingest MedQuAD XML â†’ tidy DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90184d90",
   "metadata": {
    "id": "90184d90",
    "outputId": "b231de61-f872-4d97-bf1b-cea77964cdee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Forcing re-parsing of MedQuAD XML to apply exclusion filters.\n",
      "The old medquad.parquet file will be overwritten with the filtered data.\n",
      "\n",
      "--- NEW DATASET SUMMARY ---\n",
      "Total Curated QA Pairs Stored in df: 16407\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>host</th>\n",
       "      <th>focus</th>\n",
       "      <th>focus_cui</th>\n",
       "      <th>focus_sem_type</th>\n",
       "      <th>focus_sem_group</th>\n",
       "      <th>pid</th>\n",
       "      <th>qid</th>\n",
       "      <th>qtype</th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>xml_path</th>\n",
       "      <th>q_len</th>\n",
       "      <th>a_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000559</td>\n",
       "      <td>GHR</td>\n",
       "      <td>https://ghr.nlm.nih.gov/condition/keratoderma-...</td>\n",
       "      <td>ghr.nlm.nih.gov</td>\n",
       "      <td>keratoderma with woolly hair</td>\n",
       "      <td>C0343073</td>\n",
       "      <td>T047</td>\n",
       "      <td>Disorders</td>\n",
       "      <td>1</td>\n",
       "      <td>0000559-1</td>\n",
       "      <td>information</td>\n",
       "      <td>What is (are) keratoderma with woolly hair ?</td>\n",
       "      <td>Keratoderma with woolly hair is a group of rel...</td>\n",
       "      <td>/Users/ashwinshanmugam/Documents/UIUC/Universi...</td>\n",
       "      <td>8</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000559</td>\n",
       "      <td>GHR</td>\n",
       "      <td>https://ghr.nlm.nih.gov/condition/keratoderma-...</td>\n",
       "      <td>ghr.nlm.nih.gov</td>\n",
       "      <td>keratoderma with woolly hair</td>\n",
       "      <td>C0343073</td>\n",
       "      <td>T047</td>\n",
       "      <td>Disorders</td>\n",
       "      <td>2</td>\n",
       "      <td>0000559-2</td>\n",
       "      <td>frequency</td>\n",
       "      <td>How many people are affected by keratoderma wi...</td>\n",
       "      <td>Keratoderma with woolly hair is rare; its prev...</td>\n",
       "      <td>/Users/ashwinshanmugam/Documents/UIUC/Universi...</td>\n",
       "      <td>11</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000559</td>\n",
       "      <td>GHR</td>\n",
       "      <td>https://ghr.nlm.nih.gov/condition/keratoderma-...</td>\n",
       "      <td>ghr.nlm.nih.gov</td>\n",
       "      <td>keratoderma with woolly hair</td>\n",
       "      <td>C0343073</td>\n",
       "      <td>T047</td>\n",
       "      <td>Disorders</td>\n",
       "      <td>3</td>\n",
       "      <td>0000559-3</td>\n",
       "      <td>genetic changes</td>\n",
       "      <td>What are the genetic changes related to kerato...</td>\n",
       "      <td>Mutations in the JUP, DSP, DSC2, and KANK2 gen...</td>\n",
       "      <td>/Users/ashwinshanmugam/Documents/UIUC/Universi...</td>\n",
       "      <td>12</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc_id source                                                url  \\\n",
       "0  0000559    GHR  https://ghr.nlm.nih.gov/condition/keratoderma-...   \n",
       "1  0000559    GHR  https://ghr.nlm.nih.gov/condition/keratoderma-...   \n",
       "2  0000559    GHR  https://ghr.nlm.nih.gov/condition/keratoderma-...   \n",
       "\n",
       "              host                         focus focus_cui focus_sem_type  \\\n",
       "0  ghr.nlm.nih.gov  keratoderma with woolly hair  C0343073           T047   \n",
       "1  ghr.nlm.nih.gov  keratoderma with woolly hair  C0343073           T047   \n",
       "2  ghr.nlm.nih.gov  keratoderma with woolly hair  C0343073           T047   \n",
       "\n",
       "  focus_sem_group pid        qid            qtype  \\\n",
       "0       Disorders   1  0000559-1      information   \n",
       "1       Disorders   2  0000559-2        frequency   \n",
       "2       Disorders   3  0000559-3  genetic changes   \n",
       "\n",
       "                                            question  \\\n",
       "0       What is (are) keratoderma with woolly hair ?   \n",
       "1  How many people are affected by keratoderma wi...   \n",
       "2  What are the genetic changes related to kerato...   \n",
       "\n",
       "                                              answer  \\\n",
       "0  Keratoderma with woolly hair is a group of rel...   \n",
       "1  Keratoderma with woolly hair is rare; its prev...   \n",
       "2  Mutations in the JUP, DSP, DSC2, and KANK2 gen...   \n",
       "\n",
       "                                            xml_path  q_len  a_len  \n",
       "0  /Users/ashwinshanmugam/Documents/UIUC/Universi...      8    290  \n",
       "1  /Users/ashwinshanmugam/Documents/UIUC/Universi...     11     80  \n",
       "2  /Users/ashwinshanmugam/Documents/UIUC/Universi...     12    284  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse_medquad_repo(repo_root):\n",
    "    # These are the directory names corresponding to the non-curated subsets.\n",
    "    EXCLUDE_DIRS = [\n",
    "        \"10_MPlus_ADAM_QA\",          # A.D.A.M. Medical Encyclopedia\n",
    "        \"11_MPlusDrugs_QA\",          # MedlinePlus Drug information\n",
    "        \"12_MPlusHerbsSupplements_QA\" # MedlinePlus Herbal medicine and supplement information\n",
    "    ]\n",
    "    rows = []\n",
    "\n",
    "    # Iterate through all XML files recursively\n",
    "    for xml_path in glob.glob(os.path.join(repo_root, \"**\", \"*.xml\"), recursive=True):\n",
    "\n",
    "        # --- NEW CODE: Check if the path contains an excluded directory name ---\n",
    "        if any(f\"/{d}/\" in xml_path for d in EXCLUDE_DIRS):\n",
    "             continue # Skip this XML file and move to the next one\n",
    "        # --- END NEW CODE ---\n",
    "\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "        except Exception as e:\n",
    "            print(\"Parse error:\", xml_path, e); continue\n",
    "\n",
    "        root = tree.getroot()\n",
    "        doc_id = root.attrib.get(\"id\")\n",
    "        source = root.attrib.get(\"source\")\n",
    "        url = root.attrib.get(\"url\"); host = urlparse(url).netloc if url else None\n",
    "        focus = (root.findtext(\"./Focus\") or \"\").strip()\n",
    "        cui = root.findtext(\"./FocusAnnotations/UMLS/CUIs/CUI\")\n",
    "        sem_type = root.findtext(\"./FocusAnnotations/UMLS/SemanticTypes/SemanticType\")\n",
    "        sem_group = root.findtext(\"./FocusAnnotations/UMLS/SemanticGroup\")\n",
    "\n",
    "        for qa in root.findall(\"./QAPairs/QAPair\"):\n",
    "            pid = qa.attrib.get(\"pid\")\n",
    "            q = qa.find(\"./Question\"); a = qa.find(\"./Answer\")\n",
    "            qid = q.attrib.get(\"qid\") if q is not None else None\n",
    "            qtype = q.attrib.get(\"qtype\") if q is not None else None\n",
    "            question = (q.text or \"\").strip() if q is not None else \"\"\n",
    "            answer = (a.text or \"\").strip() if a is not None else \"\"\n",
    "\n",
    "            # Normalization (Whitespace cleanup)\n",
    "            question = re.sub(r\"\\s+\", \" \", question)\n",
    "            answer = re.sub(r\"\\s+\", \" \", answer)\n",
    "\n",
    "            # --- FINAL FILTER: Skip if answer is confirmed blank (for safety) ---\n",
    "            if not answer.strip():\n",
    "                 continue\n",
    "\n",
    "            rows.append(dict(doc_id=doc_id, source=source, url=url, host=host,\n",
    "                             focus=focus, focus_cui=cui, focus_sem_type=sem_type, focus_sem_group=sem_group,\n",
    "                             pid=pid, qid=qid, qtype=qtype, question=question, answer=answer, xml_path=xml_path))\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# First, ensure the directory exists for saving the new file\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "print(\"WARNING: Forcing re-parsing of MedQuAD XML to apply exclusion filters.\")\n",
    "print(\"The old medquad.parquet file will be overwritten with the filtered data.\")\n",
    "\n",
    "# 1. CALL THE FUNCTION, APPLYING THE NEW FILTERS\n",
    "df = parse_medquad_repo(MEDQUAD_ROOT)\n",
    "\n",
    "# 2. Add Length Columns for final filtering and EDA (This prevents the old KeyError)\n",
    "df[\"q_len\"] = df[\"question\"].str.split().str.len()\n",
    "df[\"a_len\"] = df[\"answer\"].str.split().str.len()\n",
    "\n",
    "# 3. Apply the final content filter (optional but safe)\n",
    "df = df[df['a_len'] > 0].copy()\n",
    "\n",
    "# 4. OVERWRITE AND SAVE THE NEW, CLEANED DATASET\n",
    "df.to_parquet(MEDQUAD_PARQUET)\n",
    "\n",
    "print(\"\\n--- NEW DATASET SUMMARY ---\")\n",
    "print(f\"Total Curated QA Pairs Stored in df: {len(df)}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e865a",
   "metadata": {
    "id": "2a1e865a"
   },
   "source": [
    "## 2) Quick EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6359e117",
   "metadata": {
    "id": "6359e117",
    "outputId": "122840c8-0896-40aa-8cba-fdb832a9551c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top sources:\n",
      " source             host                     \n",
      "GHR                ghr.nlm.nih.gov              5430\n",
      "GARD               rarediseases.info.nih.gov    5389\n",
      "NIDDK              www.niddk.nih.gov            1192\n",
      "NINDS              www.ninds.nih.gov            1088\n",
      "MPlusHealthTopics  www.nlm.nih.gov               981\n",
      "NIHSeniorHealth    nihseniorhealth.gov           769\n",
      "CancerGov          www.cancer.gov                729\n",
      "NHLBI              www.nhlbi.nih.gov             559\n",
      "CDC                www.cdc.gov                   270\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Top question types:\n",
      " qtype\n",
      "information        4535\n",
      "symptoms           2748\n",
      "treatment          2442\n",
      "inheritance        1446\n",
      "frequency          1120\n",
      "genetic changes    1087\n",
      "causes              727\n",
      "exams and tests     653\n",
      "research            395\n",
      "outlook             361\n",
      "susceptibility      324\n",
      "considerations      235\n",
      "prevention          210\n",
      "stages               77\n",
      "complications        46\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns, matplotlib.pyplot as plt\n",
    "sns.set_theme()\n",
    "\n",
    "print(\"Top sources:\\n\", df[[\"source\",\"host\"]].value_counts().head(10))\n",
    "print(\"\\nTop question types:\\n\", df[\"qtype\"].value_counts().head(15))\n",
    "\n",
    "df[\"q_len\"] = df[\"question\"].str.split().str.len()\n",
    "df[\"a_len\"] = df[\"answer\"].str.split().str.len()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.kdeplot(df[\"q_len\"].clip(0,300), label=\"question\", ax=ax)\n",
    "sns.kdeplot(df[\"a_len\"].clip(0,600), label=\"answer\", ax=ax)\n",
    "ax.set_title(\"Token-length distributions\")\n",
    "ax.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f02ad",
   "metadata": {
    "id": "e67f02ad"
   },
   "source": [
    "## 3) Leak-safe train/dev/test splits (grouped by CUI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88936232",
   "metadata": {
    "id": "88936232",
    "outputId": "69c5ec68-20ca-4331-dd14-5ec5a7983fa3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12850, 1739, 1818)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "def make_splits(df, group_col=\"focus_cui\", seed=42):\n",
    "    df = df.copy()\n",
    "    groups = df[group_col].fillna(df[\"focus\"]).fillna(\"NOFOCUS\")\n",
    "    gss = GroupShuffleSplit(n_splits=1, train_size=0.8, random_state=seed)\n",
    "    train_idx, temp_idx = next(gss.split(df, groups=groups))\n",
    "    train = df.iloc[train_idx]; temp = df.iloc[temp_idx]\n",
    "    gss2 = GroupShuffleSplit(n_splits=1, train_size=0.5, random_state=seed)\n",
    "    dev_idx, test_idx = next(gss2.split(temp, groups=temp[group_col].fillna(temp[\"focus\"]).fillna(\"NOFOCUS\")))\n",
    "    dev = temp.iloc[dev_idx]; test = temp.iloc[test_idx]\n",
    "    return train, dev, test\n",
    "\n",
    "train_df, dev_df, test_df = make_splits(df)\n",
    "len(train_df), len(dev_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a62e3649",
   "metadata": {
    "id": "a62e3649",
    "outputId": "0c18ae52-ebb4-45c9-fc0c-1d69510c4755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total QA Pairs Parsed: 16407\n",
      "Rows with Blank Question/Answer: 0\n",
      "\n",
      "Sample of Cleaned Answer Lengths (words):\n",
      "count    16407.000000\n",
      "mean       201.354361\n",
      "std        248.480189\n",
      "min          1.000000\n",
      "25%         71.000000\n",
      "50%        138.000000\n",
      "75%        252.000000\n",
      "max       4281.000000\n",
      "Name: a_len, dtype: float64\n",
      "\n",
      "Number of QA Pairs with 1-3 Word Answers: 1\n",
      "Example of a very short answer: [['How to prevent Acanthamoeba - Granulomatous Amebic Encephalitis (GAE); Keratitis ?'\n",
      "  'Topics']]\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have run the XML parsing (Cell 4) to create 'df'\n",
    "\n",
    "# Check the number of rows that had blank answers *before* dropping them:\n",
    "total_rows = len(df)\n",
    "valid_rows = len(df.dropna(subset=['question', 'answer']))\n",
    "rows_with_missing_data = total_rows - valid_rows\n",
    "\n",
    "print(f\"Total QA Pairs Parsed: {total_rows}\")\n",
    "print(f\"Rows with Blank Question/Answer: {rows_with_missing_data}\")\n",
    "\n",
    "# Display a sample of the cleaned-up answers to check for quality\n",
    "print(\"\\nSample of Cleaned Answer Lengths (words):\")\n",
    "print(df['a_len'].describe())\n",
    "\n",
    "# Check for rows where the answer might be very short (e.g., just punctuation)\n",
    "short_answers = df[df['a_len'].between(1, 3)]\n",
    "print(f\"\\nNumber of QA Pairs with 1-3 Word Answers: {len(short_answers)}\")\n",
    "if not short_answers.empty:\n",
    "    print(\"Example of a very short answer:\", short_answers[['question', 'answer']].head(1).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d42c378",
   "metadata": {
    "id": "6d42c378"
   },
   "source": [
    "## 4) Task 1 â€” Abstractive Answer Generation (T5-small baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c1c5a72",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "9d68f0c087734204864370691c68df71",
      "16499ac23d68413b97d87ee4863942d4",
      "4e140a8f1e184bc38567377a7ac2ff3d"
     ]
    },
    "id": "9c1c5a72",
    "outputId": "16cb1ee5-1275-496b-aab0-1260f3c90440"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 12850/12850 [00:07<00:00, 1744.15 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1739/1739 [00:01<00:00, 1683.32 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1818/1818 [00:01<00:00, 1552.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast, DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from evaluate import load as load_metric\n",
    "import numpy as np, torch\n",
    "\n",
    "def build_hf_dataset(train_df, dev_df, test_df, max_input=512, max_target=512):\n",
    "    def to_ds(frame):\n",
    "        sub = frame[[\"question\",\"answer\"]].dropna().rename(columns={\"question\":\"input_text\",\"answer\":\"target_text\"})\n",
    "        return Dataset.from_pandas(sub)\n",
    "    raw = DatasetDict(train=to_ds(train_df), validation=to_ds(dev_df), test=to_ds(test_df))\n",
    "    tok = T5TokenizerFast.from_pretrained(\"t5-small\")\n",
    "    def preprocess(ex):\n",
    "        ex[\"input_ids\"] = tok(\"question: \" + ex[\"input_text\"], truncation=True, max_length=max_input).input_ids\n",
    "        ex[\"labels\"]    = tok(ex[\"target_text\"], truncation=True, max_length=max_target).input_ids\n",
    "        return ex\n",
    "    tokenized = raw.map(preprocess, remove_columns=raw[\"train\"].column_names)\n",
    "    return tokenized, tok\n",
    "\n",
    "tokenized, tok = build_hf_dataset(train_df, dev_df, test_df)\n",
    "\n",
    "def train_t5(tokenized, tok, out_dir=\"./t5-medquad\"):\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "    collator = DataCollatorForSeq2Seq(tok, model=model)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=out_dir, per_device_train_batch_size=2, per_device_eval_batch_size=2,\n",
    "        learning_rate=3e-4, num_train_epochs=5, fp16=False, eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\", logging_steps=50, report_to=\"none\"\n",
    "    )\n",
    "    trainer = Trainer(model=model, args=args, data_collator=collator,\n",
    "                      train_dataset=tokenized[\"train\"], eval_dataset=tokenized[\"validation\"])\n",
    "    trainer.train()\n",
    "    return trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faa898e3",
   "metadata": {
    "id": "faa898e3"
   },
   "outputs": [],
   "source": [
    "# Clear Memory Cache\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74943817",
   "metadata": {
    "id": "74943817",
    "outputId": "939d1e28-dfe6-4045-c55f-befbb8f4b6ea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='32125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   88/32125 00:36 < 3:44:24, 2.38 it/s, Epoch 0.01/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 3.32 GiB, other allocations: 5.63 GiB, max allowed: 9.07 GiB). Tried to allocate 125.50 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_t5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtok\u001b[49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# â† Uncomment to train\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./t5-medquad\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 31\u001b[0m, in \u001b[0;36mtrain_t5\u001b[0;34m(tokenized, tok, out_dir)\u001b[0m\n\u001b[1;32m     24\u001b[0m args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[1;32m     25\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39mout_dir, per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, per_device_eval_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     26\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3e-4\u001b[39m, num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, fp16\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, eval_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     27\u001b[0m     save_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, logging_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m, report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[1;32m     29\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model\u001b[38;5;241m=\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39margs, data_collator\u001b[38;5;241m=\u001b[39mcollator,\n\u001b[1;32m     30\u001b[0m                   train_dataset\u001b[38;5;241m=\u001b[39mtokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m], eval_dataset\u001b[38;5;241m=\u001b[39mtokenized[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 31\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\n",
      "File \u001b[0;32m~/miniconda3/envs/medquad_env/lib/python3.10/site-packages/transformers/trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/medquad_env/lib/python3.10/site-packages/transformers/trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[1;32m   2672\u001b[0m )\n\u001b[1;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[0;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[1;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2680\u001b[0m ):\n\u001b[1;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/medquad_env/lib/python3.10/site-packages/transformers/trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[1;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m-> 4071\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/medquad_env/lib/python3.10/site-packages/accelerate/accelerator.py:2740\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2738\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[1;32m   2739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2740\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/medquad_env/lib/python3.10/site-packages/torch/_tensor.py:647\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    639\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    640\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    646\u001b[0m     )\n\u001b[0;32m--> 647\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/medquad_env/lib/python3.10/site-packages/torch/autograd/__init__.py:354\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    349\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/medquad_env/lib/python3.10/site-packages/torch/autograd/graph.py:829\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 3.32 GiB, other allocations: 5.63 GiB, max allowed: 9.07 GiB). Tried to allocate 125.50 MiB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "trainer = train_t5(tokenized, tok)   # â† Uncomment to train\n",
    "trainer.save_model(\"./t5-medquad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d35eb8",
   "metadata": {
    "id": "e3d35eb8"
   },
   "outputs": [],
   "source": [
    "def eval_generation(model_dir, tokenized, tok):\n",
    "    # This function is assumed to be run after the model has been trained and saved to model_dir\n",
    "    import torch, numpy as np\n",
    "    from evaluate import load as load_metric\n",
    "    from transformers import T5ForConditionalGeneration # Note: Use AutoModelForSeq2SeqLM if using BioBART\n",
    "\n",
    "    rouge = load_metric(\"rouge\")\n",
    "    bertscore = load_metric(\"bertscore\")\n",
    "\n",
    "    # Load the model and ensure it uses the M4 GPU\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
    "    DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # Select a subset of the test data (e.g., 256 examples)\n",
    "    test = tokenized[\"test\"].select(range(min(256, len(tokenized[\"test\"]))))\n",
    "\n",
    "    # --- Generation Logic (Assumed Correct and Working) ---\n",
    "    input_ids_list = test[\"input_ids\"]\n",
    "    padded_inputs = tok.pad(\n",
    "        {'input_ids': input_ids_list},\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids.to(DEVICE)\n",
    "\n",
    "    preds = model.generate(padded_inputs, max_new_tokens=256)\n",
    "\n",
    "    pred_text = tok.batch_decode(preds, skip_special_tokens=True)\n",
    "    ref_text  = tok.batch_decode(test[\"labels\"], skip_special_tokens=True)\n",
    "\n",
    "    # --- Metrics Calculation ---\n",
    "    r = rouge.compute(predictions=pred_text, references=ref_text, use_stemmer=True)\n",
    "    b = bertscore.compute(predictions=pred_text, references=ref_text, lang=\"en\")\n",
    "\n",
    "    # FINAL FIX: Access the ROUGE-L score directly as the key's value.\n",
    "    print(\"ROUGE-L:\", r[\"rougeL\"], \" BERTScore(F1):\", float(np.mean(b[\"f1\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289cd50-a072-4dda-9924-389043271e0f",
   "metadata": {
    "id": "f289cd50-a072-4dda-9924-389043271e0f",
    "outputId": "928dcfe4-8ab0-411a-cab3-18031ebb9121"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-L: 0.4064322101893944  BERTScore(F1): 0.884045472368598\n"
     ]
    }
   ],
   "source": [
    "# Assuming your trained model was saved to the T5 folder as planned\n",
    "# NOTE: If you trained BioBART, you must change the directory name.\n",
    "\n",
    "eval_generation(\n",
    "    model_dir=\"./t5-medquad\",     # The directory where trainer.save_model() stored the weights\n",
    "    tokenized=tokenized,          # The DatasetDict containing the 'test' set\n",
    "    tok=tok                       # The T5TokenizerFast object\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb07bb-e9ac-4b3c-b326-5975e8faaee9",
   "metadata": {
    "id": "efbb07bb-e9ac-4b3c-b326-5975e8faaee9",
    "outputId": "0133e334-3f17-4cfb-abf6-6e6070ad8422"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qualitative Results (Top 10 Examples):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Candidate_Answer</th>\n",
       "      <th>Reference_Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is (are) keratoderma with woolly hair?</td>\n",
       "      <td>Keratoderma with woolly hair is a skin conditi...</td>\n",
       "      <td>Keratoderma with woolly hair is a group of rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How many people are affected by keratoderma wi...</td>\n",
       "      <td>Keratoderma with woolly hair is a rare conditi...</td>\n",
       "      <td>Keratoderma with woolly hair is rare; its prev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the genetic changes related to kerato...</td>\n",
       "      <td>Keratoderma with woolly hair is caused by muta...</td>\n",
       "      <td>Mutations in the JUP, DSP, DSC2, and KANK2 gen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is keratoderma with woolly hair inherited?</td>\n",
       "      <td>This condition is inherited in an autosomal re...</td>\n",
       "      <td>Most cases of keratoderma with woolly hair hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the treatments for keratoderma with w...</td>\n",
       "      <td>These resources address the diagnosis or manag...</td>\n",
       "      <td>These resources address the diagnosis or manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is (are) trisomy 18?</td>\n",
       "      <td>Trisomy 18 is an inherited disorder that affec...</td>\n",
       "      <td>Trisomy 18, also called Edwards syndrome, is a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How many people are affected by trisomy 18?</td>\n",
       "      <td>Trisomy 18 is a rare disorder; its prevalence ...</td>\n",
       "      <td>Trisomy 18 occurs in about 1 in 5,000 live-bor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What are the genetic changes related to trisom...</td>\n",
       "      <td>Trisomy 18 is caused by mutations in the COL1A...</td>\n",
       "      <td>Most cases of trisomy 18 result from having th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Is trisomy 18 inherited?</td>\n",
       "      <td>This condition is inherited in an autosomal re...</td>\n",
       "      <td>Most cases of trisomy 18 are not inherited, bu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What are the treatments for trisomy 18?</td>\n",
       "      <td>These resources address the diagnosis or manag...</td>\n",
       "      <td>These resources address the diagnosis or manag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Question  \\\n",
       "0        What is (are) keratoderma with woolly hair?   \n",
       "1  How many people are affected by keratoderma wi...   \n",
       "2  What are the genetic changes related to kerato...   \n",
       "3         Is keratoderma with woolly hair inherited?   \n",
       "4  What are the treatments for keratoderma with w...   \n",
       "5                          What is (are) trisomy 18?   \n",
       "6        How many people are affected by trisomy 18?   \n",
       "7  What are the genetic changes related to trisom...   \n",
       "8                           Is trisomy 18 inherited?   \n",
       "9            What are the treatments for trisomy 18?   \n",
       "\n",
       "                                    Candidate_Answer  \\\n",
       "0  Keratoderma with woolly hair is a skin conditi...   \n",
       "1  Keratoderma with woolly hair is a rare conditi...   \n",
       "2  Keratoderma with woolly hair is caused by muta...   \n",
       "3  This condition is inherited in an autosomal re...   \n",
       "4  These resources address the diagnosis or manag...   \n",
       "5  Trisomy 18 is an inherited disorder that affec...   \n",
       "6  Trisomy 18 is a rare disorder; its prevalence ...   \n",
       "7  Trisomy 18 is caused by mutations in the COL1A...   \n",
       "8  This condition is inherited in an autosomal re...   \n",
       "9  These resources address the diagnosis or manag...   \n",
       "\n",
       "                                    Reference_Answer  \n",
       "0  Keratoderma with woolly hair is a group of rel...  \n",
       "1  Keratoderma with woolly hair is rare; its prev...  \n",
       "2  Mutations in the JUP, DSP, DSC2, and KANK2 gen...  \n",
       "3  Most cases of keratoderma with woolly hair hav...  \n",
       "4  These resources address the diagnosis or manag...  \n",
       "5  Trisomy 18, also called Edwards syndrome, is a...  \n",
       "6  Trisomy 18 occurs in about 1 in 5,000 live-bor...  \n",
       "7  Most cases of trisomy 18 result from having th...  \n",
       "8  Most cases of trisomy 18 are not inherited, bu...  \n",
       "9  These resources address the diagnosis or manag...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_qualitative_results(model_dir, tokenized, tok, num_examples=10):\n",
    "    \"\"\"\n",
    "    Loads the trained model, generates predictions, and returns a DataFrame\n",
    "    showing the Question, Model Answer, and Reference Answer.\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    import pandas as pd\n",
    "    from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "    # 1. Load Model and Set Device\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
    "    DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model.to(DEVICE)\n",
    "\n",
    "    # 2. Select Test Subset (This is where the data is loaded)\n",
    "    test_subset = tokenized[\"test\"].select(range(num_examples))\n",
    "\n",
    "    # 3. Prepare Input IDs with Padding\n",
    "    input_ids_list = test_subset[\"input_ids\"]\n",
    "    padded_inputs = tok.pad(\n",
    "        {'input_ids': input_ids_list},\n",
    "        return_tensors=\"pt\"\n",
    "    ).input_ids.to(DEVICE)\n",
    "\n",
    "    # 4. Generate Predictions\n",
    "    preds = model.generate(\n",
    "        padded_inputs,\n",
    "        max_new_tokens=256,\n",
    "        num_beams=4,\n",
    "        do_sample=False\n",
    "    )\n",
    "\n",
    "    # 5. Decode Results (The Fix is here)\n",
    "    pred_text = tok.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # --- FIX START: Restore Question Text from Tokens ---\n",
    "    # a. Decode the input tokens (which contain the text + \"question: \" prefix)\n",
    "    questions_with_prefix = tok.batch_decode(test_subset[\"input_ids\"], skip_special_tokens=True)\n",
    "    # b. Strip the \"question: \" prefix (T5 instruction) for a clean display\n",
    "    questions = [q.replace('question: ', '').strip() for q in questions_with_prefix]\n",
    "    # c. Decode the reference labels\n",
    "    ref_text  = tok.batch_decode(test_subset[\"labels\"], skip_special_tokens=True)\n",
    "    # --- FIX END ---\n",
    "\n",
    "    # 6. Compile and Return DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'Question': questions,\n",
    "        'Candidate_Answer': pred_text,\n",
    "        'Reference_Answer': ref_text\n",
    "    })\n",
    "    return results_df\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# NOTE: Use \"./biobart-medquad\" if you trained the BioBART model.\n",
    "qual_df = get_qualitative_results(\n",
    "    model_dir=\"./t5-medquad\",\n",
    "    tokenized=tokenized,\n",
    "    tok=tok,\n",
    "    num_examples=10\n",
    ")\n",
    "\n",
    "print(f\"Qualitative Results (Top 10 Examples):\\n\")\n",
    "qual_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6319786",
   "metadata": {
    "id": "b6319786"
   },
   "source": [
    "## 5) Task 2 â€” Paraphrase Detection (Siamese baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfc3e33",
   "metadata": {
    "id": "fbfc3e33",
    "outputId": "acdf060f-5f52-4b04-c05a-dc6c0c996d2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26280, 6570, np.float64(0.60882800608828))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def build_paraphrase_pairs(df, max_pos=20000, max_neg=20000, seed=13):\n",
    "    df2 = df.dropna(subset=[\"question\"]).copy()\n",
    "    pos = []\n",
    "    for cui, block in df2.groupby(df2[\"focus_cui\"].fillna(\"NOFOCUS\")):\n",
    "        qs = block[\"question\"].drop_duplicates().tolist()\n",
    "        for a,b in combinations(qs, 2):\n",
    "            pos.append((a,b,1))\n",
    "            if len(pos) >= max_pos: break\n",
    "        if len(pos) >= max_pos: break\n",
    "    a_list = df2[\"question\"].sample(min(max_neg, len(df2)), random_state=seed).tolist()\n",
    "    b_list = df2[\"question\"].sample(min(max_neg, len(df2)), random_state=seed+1).tolist()\n",
    "    neg = [(a,b,0) for a,b in zip(a_list,b_list) if a!=b][:max_neg]\n",
    "    pairs = pd.DataFrame(pos+neg, columns=[\"q1\",\"q2\",\"label\"]).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    return pairs\n",
    "\n",
    "pairs = build_paraphrase_pairs(train_df)\n",
    "pairs_train, pairs_dev = train_test_split(pairs, test_size=0.2, random_state=7, stratify=pairs[\"label\"])\n",
    "len(pairs_train), len(pairs_dev), pairs_train[\"label\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72a5947",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "14ae04db270d4f0b9ff6787b21cc006c"
     ]
    },
    "id": "d72a5947",
    "outputId": "64daae94-c9bb-44d3-8139-dbb9b190b48d"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14ae04db270d4f0b9ff6787b21cc006c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='822' max='822' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [822/822 01:26, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.041200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import torch, numpy as np\n",
    "\n",
    "def train_siamese(pairs, model_name=\"sentence-transformers/all-MiniLM-L6-v2\", epochs=1):\n",
    "    model = SentenceTransformer(model_name)\n",
    "    train_ex = [InputExample(texts=[r.q1, r.q2], label=float(r.label)) for _,r in pairs.iterrows()]\n",
    "    loader = DataLoader(train_ex, shuffle=True, batch_size=32)\n",
    "    train_loss = losses.CosineSimilarityLoss(model)\n",
    "    model.fit(train_objectives=[(loader, train_loss)], epochs=epochs, warmup_steps=max(1, int(len(loader)*0.1)))\n",
    "    return model\n",
    "\n",
    "sbert = train_siamese(pairs_train)        # â† Uncomment to train\n",
    "sbert.save(\"./sbert-paraphrase-medquad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5414b8",
   "metadata": {
    "id": "6d5414b8",
    "outputId": "51c238af-55a7-4954-b97a-5dcb18879a15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9894977168949771"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def eval_siamese(model_dir, eval_df, thresh=0.5):\n",
    "    model = SentenceTransformer(model_dir)\n",
    "    a = model.encode(eval_df[\"q1\"].tolist(), convert_to_tensor=True, show_progress_bar=False)\n",
    "    b = model.encode(eval_df[\"q2\"].tolist(), convert_to_tensor=True, show_progress_bar=False)\n",
    "    sims = torch.nn.functional.cosine_similarity(a,b).cpu().numpy()\n",
    "    preds = (sims >= thresh).astype(int)\n",
    "    acc = (preds == eval_df[\"label\"].values).mean()\n",
    "    return float(acc)\n",
    "\n",
    "eval_siamese(\"./sbert-paraphrase-medquad\", pairs_dev)  # â† Evaluate after training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bbce330",
   "metadata": {
    "id": "2bbce330"
   },
   "source": [
    "## 6) Task 3 â€” RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "149a8ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6084f",
   "metadata": {},
   "source": [
    "## Simple RAG Implementation for Beginners\n",
    "\n",
    "This section provides a simplified, beginner-friendly implementation of Retrieval-Augmented Generation (RAG) that builds upon the existing complex RAG system in the notebook. The implementation includes:\n",
    "\n",
    "1. **Modular Design**: Easy to understand and modify components\n",
    "2. **Multiple Retrieval Methods**: BM25, Dense, and Hybrid approaches\n",
    "3. **Simple Generation**: Straightforward T5-based answer generation\n",
    "4. **Clear Documentation**: Step-by-step explanations for beginners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fa01ad3-0c78-46fe-991e-c0b0887022b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6ff41978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "class RAG:\n",
    "    def __init__(self, model_name=\"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initialize a simple RAG system\n",
    "        \n",
    "        Args:\n",
    "            model_name: Sentence transformer model for dense retrieval\n",
    "        \"\"\"\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.dense_encoder = SentenceTransformer(model_name)\n",
    "        self.bm25 = None\n",
    "        self.corpus = []\n",
    "        self.corpus_embeddings = None\n",
    "        self.generator_tokenizer = None\n",
    "        self.generator_model = None\n",
    "        \n",
    "    def build_retrieval_index(self, documents):\n",
    "        \"\"\"\n",
    "        Build both BM25 and dense retrieval indices\n",
    "        \n",
    "        Args:\n",
    "            documents: List of document texts to index\n",
    "        \"\"\"\n",
    "        print(\"Building retrieval indices...\")\n",
    "        \n",
    "        from rank_bm25 import BM25Okapi\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        # Store corpus\n",
    "        self.corpus = documents\n",
    "        \n",
    "        # Build BM25 index\n",
    "        tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]\n",
    "        self.bm25 = BM25Okapi(tokenized_docs)\n",
    "        \n",
    "        # Build dense embeddings\n",
    "        self.corpus_embeddings = self.dense_encoder.encode(\n",
    "            documents, \n",
    "            convert_to_tensor=True,\n",
    "            show_progress_bar=True\n",
    "        )\n",
    "        \n",
    "        print(f\"Indexed {len(documents)} documents\")\n",
    "        \n",
    "    def load_generator(self, model_name=\"t5-small\"):\n",
    "        \"\"\"\n",
    "        Load the text generation model\n",
    "        \n",
    "        Args:\n",
    "            model_name: HuggingFace model name for generation\n",
    "        \"\"\"\n",
    "        from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "        print(f\"Loading generator: {model_name}\")\n",
    "        self.generator_tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "        self.generator_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        print(\"Generator loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4cf4d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reached end of cell\n"
     ]
    }
   ],
   "source": [
    "# Add retrieval methods to the RAG class\n",
    "def add_retrieval_methods():\n",
    "    \"\"\"Add retrieval methods to the RAG class\"\"\"\n",
    "    \n",
    "    def retrieve_documents(self, query, k=5, method=\"hybrid\"):\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: Input question\n",
    "            k: Number of documents to retrieve\n",
    "            method: \"bm25\", \"dense\", or \"hybrid\"\n",
    "            \n",
    "        Returns:\n",
    "            List of retrieved document indices and texts\n",
    "        \"\"\"\n",
    "        if method == \"bm25\":\n",
    "            return self._bm25_retrieve(query, k)\n",
    "        elif method == \"dense\":\n",
    "            return self._dense_retrieve(query, k)\n",
    "        elif method == \"hybrid\":\n",
    "            return self._hybrid_retrieve(query, k)\n",
    "        else:\n",
    "            raise ValueError(\"Method must be 'bm25', 'dense', or 'hybrid'\")\n",
    "    \n",
    "    def _bm25_retrieve(self, query, k):\n",
    "        \"\"\"BM25 retrieval\"\"\"\n",
    "        import numpy as np\n",
    "        from nltk.tokenize import word_tokenize\n",
    "        query_tokens = word_tokenize(query.lower())\n",
    "        scores = self.bm25.get_scores(query_tokens)\n",
    "        top_indices = np.argsort(scores)[-k:][::-1]\n",
    "        return [(idx, self.corpus[idx], scores[idx]) for idx in top_indices]\n",
    "    \n",
    "    def _dense_retrieve(self, query, k):\n",
    "        \"\"\"Dense retrieval using sentence transformers\"\"\"\n",
    "        import torch\n",
    "        query_embedding = self.dense_encoder.encode([query], convert_to_tensor=True)\n",
    "        # Compute cosine similarity between query (D,) and corpus (N,D) via normalized dot product\n",
    "        q = torch.nn.functional.normalize(query_embedding.squeeze(0), dim=0)\n",
    "        C = torch.nn.functional.normalize(self.corpus_embeddings, dim=1)\n",
    "        sim = torch.matmul(C, q)  # (N,)\n",
    "        top_indices = torch.topk(sim, k).indices\n",
    "        return [(idx.item(), self.corpus[idx.item()], float(sim[idx])) for idx in top_indices]\n",
    "    \n",
    "    def _hybrid_retrieve(self, query, k):\n",
    "        \"\"\"Simple hybrid retrieval: combine BM25 and dense scores\"\"\"\n",
    "        # Get top candidates from both methods\n",
    "        bm25_results = self._bm25_retrieve(query, k*2)\n",
    "        dense_results = self._dense_retrieve(query, k*2)\n",
    "        \n",
    "        # Create score dictionary\n",
    "        scores = {}\n",
    "        for idx, text, score in bm25_results:\n",
    "            scores[idx] = scores.get(idx, 0) + score\n",
    "        for idx, text, score in dense_results:\n",
    "            scores[idx] = scores.get(idx, 0) + score\n",
    "            \n",
    "        # Sort by combined score and return top k\n",
    "        sorted_indices = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)\n",
    "        return [(idx, self.corpus[idx], scores[idx]) for idx in sorted_indices[:k]]\n",
    "    \n",
    "    # Add methods to the class\n",
    "    RAG.retrieve_documents = retrieve_documents\n",
    "    RAG._bm25_retrieve = _bm25_retrieve\n",
    "    RAG._dense_retrieve = _dense_retrieve\n",
    "    RAG._hybrid_retrieve = _hybrid_retrieve\n",
    "\n",
    "# Execute the function to add methods to the class\n",
    "add_retrieval_methods()\n",
    "print(\"reached end of cell\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ee306f5d-ae5a-45d2-8702-a213acd7718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building retrieval indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 47/47 [00:03<00:00, 11.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1500 documents\n",
      "\n",
      "Query: What are the symptoms of Diabetes insipidus nephrogenic mental retardation and intracerebral calcification?\n",
      "\n",
      "Top BM25 hits:\n",
      "1. [doc 464] score=31.7521  |  What are the treatments for Wolfram syndrome ? These resources address the diagnosis or management of Wolfram syndrome: - Gene Review: Gene Review: WFS1-Related Disorders - Genetic Testing Registry: Diabetes mellitus AND...\n",
      "2. [doc 104] score=30.5475  |  What are the treatments for Mabry syndrome ? These resources address the diagnosis or management of Mabry syndrome: - Genetic Testing Registry: Hyperphosphatasia with mental retardation syndrome - Genetic Testing Registr...\n",
      "3. [doc 1079] score=26.6279  |  What are the treatments for CASK-related intellectual disability ? These resources address the diagnosis or management of CASK-related intellectual disability: - Gene Review: Gene Review: CASK-Related Disorders - Genetic...\n",
      "\n",
      "Top Dense hits:\n",
      "1. [doc 1490] score=0.5145  |  What is (are) autosomal recessive spastic ataxia of Charlevoix-Saguenay ? Autosomal recessive spastic ataxia of Charlevoix-Saguenay, more commonly known as ARSACS, is a condition affecting muscle movement. People with AR...\n",
      "2. [doc 1030] score=0.5015  |  What is (are) Alpers-Huttenlocher syndrome ? Alpers-Huttenlocher syndrome is one of the most severe of a group of conditions called the POLG-related disorders. The conditions in this group feature a range of similar sign...\n",
      "3. [doc 665] score=0.4924  |  What is (are) combined malonic and methylmalonic aciduria ? Combined malonic and methylmalonic aciduria (CMAMMA) is a condition characterized by high levels of certain chemicals, known as malonic acid and methylmalonic a...\n",
      "\n",
      "Top Hybrid hits:\n",
      "1. [doc 464] score=31.7521  |  What are the treatments for Wolfram syndrome ? These resources address the diagnosis or management of Wolfram syndrome: - Gene Review: Gene Review: WFS1-Related Disorders - Genetic Testing Registry: Diabetes mellitus AND...\n",
      "2. [doc 104] score=30.5475  |  What are the treatments for Mabry syndrome ? These resources address the diagnosis or management of Mabry syndrome: - Genetic Testing Registry: Hyperphosphatasia with mental retardation syndrome - Genetic Testing Registr...\n",
      "3. [doc 1079] score=26.6279  |  What are the treatments for CASK-related intellectual disability ? These resources address the diagnosis or management of CASK-related intellectual disability: - Gene Review: Gene Review: CASK-Related Disorders - Genetic...\n",
      "Loading generator: t5-small\n",
      "Generator loaded successfully\n",
      "\n",
      "Generated answer (t5-small): Hyperphosphatasia with mental retardation syndrome 1 - Genetic Testing Registry: Hyperphosphatasia with mental retardation syndrome 2 - Genetic Testing Registry: Hyperphosphatasia with mental retardation syndrome 2 - Genetic Testing Registry: Hyperphosphatasia with mental retardation syndrome 2 - Genetic Testing Registry: Hyperphosphatasia with\n"
     ]
    }
   ],
   "source": [
    "# Demo: Run RAG end-to-end to show output\n",
    "def run_rag_demo():\n",
    "    # Ensure dependencies and the RAG class are available\n",
    "    import numpy as np, torch\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    from rank_bm25 import BM25Okapi\n",
    "    from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    if 'RAG' not in globals():\n",
    "        print('Please run the previous cell that defines the RAG class first.')\n",
    "        return\n",
    "\n",
    "    # 1) Prepare a small corpus from MedQuAD DataFrame (question + answer)\n",
    "    import nltk; nltk.download('punkt', quiet=True)\n",
    "    try:\n",
    "        _ = df\n",
    "    except NameError:\n",
    "        import os, pandas as pd\n",
    "        try:\n",
    "            medquad_parquet = MEDQUAD_PARQUET\n",
    "        except NameError:\n",
    "            medquad_parquet = './artifacts-medquad/medquad.parquet'\n",
    "        df = pd.read_parquet(medquad_parquet) if os.path.exists(medquad_parquet) else None\n",
    "\n",
    "    if df is None or len(df) == 0:\n",
    "        # Fallback tiny corpus if dataset not loaded\n",
    "        docs = [\n",
    "            \"Diabetes is a disease that occurs when your blood glucose is too high. Symptoms include increased thirst, frequent urination, extreme fatigue, and blurred vision.\",\n",
    "            \"Influenza (flu) is a contagious respiratory illness. Symptoms include fever, cough, sore throat, runny or stuffy nose, body aches, headaches, chills and fatigue.\",\n",
    "            \"Hypertension (high blood pressure) rarely has symptoms, but long-term it can lead to heart disease and stroke.\"\n",
    "        ]\n",
    "    else:\n",
    "        sample = df.head(1500)  # keep it small for a quick demo\n",
    "        docs = (sample['question'].astype(str) + ' ' + sample['answer'].astype(str)).tolist()\n",
    "\n",
    "    # 2) Build indices and run retrieval\n",
    "    rag = RAG()\n",
    "    rag.build_retrieval_index(docs)\n",
    "\n",
    "    query = 'What are the symptoms of Diabetes insipidus nephrogenic mental retardation and intracerebral calcification?'\n",
    "    print(f'\\nQuery: {query}')\n",
    "\n",
    "    bm25_hits = rag.retrieve_documents(query, k=3, method='bm25')\n",
    "    dense_hits = rag.retrieve_documents(query, k=3, method='dense')\n",
    "    hybrid_hits = rag.retrieve_documents(query, k=3, method='hybrid')\n",
    "\n",
    "    def show_hits(name, hits):\n",
    "        print(f'\\nTop {name} hits:')\n",
    "        for i, (idx, text, score) in enumerate(hits, 1):\n",
    "            preview = (text[:220] + '...') if len(text) > 220 else text\n",
    "            try:\n",
    "                s = f\"{float(score):.4f}\"\n",
    "            except Exception:\n",
    "                s = str(score)\n",
    "            print(f\"{i}. [doc {idx}] score={s}  |  {preview}\")\n",
    "\n",
    "    show_hits('BM25', bm25_hits)\n",
    "    show_hits('Dense', dense_hits)\n",
    "    show_hits('Hybrid', hybrid_hits)\n",
    "\n",
    "    # 3) Optional: Load generator and produce an answer from retrieved context\n",
    "    rag.load_generator('t5-small')\n",
    "    context = ' '.join([t for _, t, _ in hybrid_hits])[:1500]\n",
    "    inp = f'question: {query} context: {context}'\n",
    "    tok = rag.generator_tokenizer([inp], return_tensors='pt', padding=True, truncation=True)\n",
    "    out_ids = rag.generator_model.generate(**tok, max_new_tokens=64, do_sample=False)\n",
    "    ans = rag.generator_tokenizer.decode(out_ids[0], skip_special_tokens=True)\n",
    "    print('\\nGenerated answer (t5-small):', ans)\n",
    "\n",
    "# Execute demo\n",
    "run_rag_demo()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a33d675-35a2-40f0-b9bf-763f4a95135b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (medquad_env)",
   "language": "python",
   "name": "medquad_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
